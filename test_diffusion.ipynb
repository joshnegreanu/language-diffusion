{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "import torch\n",
    "import math\n",
    "import pandas as pd\n",
    "import random\n",
    "import wandb\n",
    "\n",
    "from torch import nn, optim\n",
    "from torch.optim.lr_scheduler import LinearLR, CosineAnnealingLR, SequentialLR\n",
    "from torch.utils.data import DataLoader, Dataset, ConcatDataset\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from staticvectors import StaticVectors\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "from models.LanguageDiffusion import LanguageDiffusion\n",
    "from data.DiffuseLanguage import DiffuseLanguageDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dynamically select device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.6319, device='mps:0', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# load trained model\n",
    "model_path = \"./checkpoints/diffusion-language-model/dlm-2025_12_19_22_12_epoch9_end.pth\"\n",
    "chkpt = torch.load(model_path, weights_only=False, map_location=torch.device(device))\n",
    "\n",
    "# get model configuration\n",
    "model_config = chkpt['model_config']\n",
    "\n",
    "# get vocabulary\n",
    "vocab = chkpt['vocab']\n",
    "print(chkpt['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LanguageDiffusion(\n",
       "  (token_embedding): Embedding(9944, 256)\n",
       "  (pos_enc): PositionalEncoding()\n",
       "  (transformer): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-15): 16 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): Linear(in_features=256, out_features=9944, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load language model\n",
    "model = LanguageDiffusion(\n",
    "    vocab_size=len(vocab),\n",
    "    embed_dim=model_config['emb_dim'],\n",
    "    num_layers=model_config['num_layers'],\n",
    "    num_heads=model_config['num_heads']\n",
    ")\n",
    "\n",
    "model.load_state_dict(chkpt['model_state_dict'])\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = {\n",
    "    'max_len': 200,\n",
    "    'sampling_steps': 100,\n",
    "    'temperature': 0.9,\n",
    "    'top_p': 0.9\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:05<00:00, 19.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lily and ben are in the park. they like to run and jump in the park. they see a big hill. they want to go inside. they climb up the hill. but it is very dangerous. it put on a big hole on the hill. they see a hole in the hole and a hole. ben puts and jump up. they move. then they shakes the hole.  but there is wrong. it runs out the cage.  \" no, but the hole is faster. \" ben says. \" let me see them. we are brave. they look at them and say, \" do n't worry, bird. we did not to hurt us. \"  \" i am a hole. lily is a bird. you are safe. how can we play with you? \"  ben smiles and says, \" you are not alert. okay. you are going to help you. we are dangerous. we hope the bird are very dangerous. \" lily smile. they hug and\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "model.eval()\n",
    "\n",
    "generated_text = torch.ones((1, generation_config['max_len']), dtype=torch.long).to(device) * vocab.word2idx['<m>']\n",
    "\n",
    "prev_masks = torch.ones_like(generated_text, dtype=torch.bool).to(device)\n",
    "prev_probs = torch.ones_like(generated_text, dtype=torch.float).to(device)\n",
    "sample_t = torch.linspace(0, 1, steps=100)\n",
    "\n",
    "# diffuse generation\n",
    "for i in tqdm(range(generation_config['sampling_steps'])):\n",
    "    t = sample_t[generation_config['sampling_steps'] - i - 1]\n",
    "\n",
    "    # potentially add other tokens to mask pool based on uncertainty of prediction\n",
    "    prev_masks = prev_masks | (prev_probs < random.uniform(0, 1))\n",
    "\n",
    "    # create mask only on masked tokens\n",
    "    mask = (torch.rand(generated_text.shape) < t).to(device) & prev_masks\n",
    "    prev_masks = prev_masks | mask.to(device)\n",
    "    masks = generated_text.masked_fill(mask.to(device), vocab.word2idx['<m>']).to(device)\n",
    "\n",
    "    out = model(masks)\n",
    "    out /= generation_config['temperature']\n",
    "\n",
    "    # probabilities\n",
    "    probs = nn.functional.softmax(out, dim=-1)\n",
    "\n",
    "    \"\"\"\n",
    "    # nucleus/top-p filtering\n",
    "    sorted_probs, sorted_indices = torch.sort(probs, descending=True, dim=-1)\n",
    "    cumulative_probs = torch.cumsum(sorted_probs, dim=-1)\n",
    "    \n",
    "    # nucleus coverage\n",
    "    nucleus = cumulative_probs < generation_config['top_p']\n",
    "    nucleus_size = torch.sum(nucleus, dim=-1)\n",
    "    nucleus[0, nucleus_size] = True\n",
    "\n",
    "    # apply coverage\n",
    "    nucleus_probs = torch.zeros_like(probs)\n",
    "    nucleus_probs = torch.where(nucleus, sorted_probs, sorted_probs)\n",
    "    nucleus_probs /= torch.sum(nucleus_probs, dim=-1, keepdim=True)\n",
    "\n",
    "    # sample from filtered distribution\n",
    "    sampled_indices = torch.multinomial(nucleus_probs.view(-1, nucleus_probs.size(-1)), num_samples=1).view(nucleus_probs.size(0), nucleus_probs.size(1)).unsqueeze(1)\n",
    "    sampled_tokens = torch.gather(sorted_indices, -1, sampled_indices).squeeze(-1)\n",
    "    generated_text = torch.where(mask, sampled_tokens, generated_text).squeeze(0).to(device)\n",
    "    \"\"\"\n",
    "\n",
    "    sampled_tokens = torch.multinomial(probs.view(-1, probs.size(-1)), num_samples=1).view(probs.size(0), probs.size(1)).to(device)\n",
    "    generated_text = torch.where(mask.to(device), sampled_tokens, generated_text).to(device)\n",
    "\n",
    "    # update previous probabilities as corresponding probabilities of sampled tokens\n",
    "    prev_probs = torch.gather(probs, -1, sampled_tokens.unsqueeze(-1)).squeeze(-1).to(device)\n",
    "\n",
    "# decode generated tokens\n",
    "generated_string = vocab.idx2text(generated_text.squeeze().tolist())\n",
    "generated_string = re.sub(r'\\s+([.,!?])', r'\\1', generated_string)\n",
    "print(generated_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mark was playing with <m> <m> and his big red ball .\n"
     ]
    }
   ],
   "source": [
    "text_input = \"mark was playing with his toys and his big red ball.\"\n",
    "generated_text = torch.tensor([vocab.text2idx(text_input)], dtype=torch.long).to(device)\n",
    "mask = (torch.rand(generated_text.shape) < 0.25).to(device)\n",
    "masked_input = generated_text.masked_fill(mask.to(device), vocab.word2idx['<m>']).to(device)\n",
    "print(vocab.idx2text(masked_input.squeeze().tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.6812, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000]], device='mps:0', grad_fn=<MaxBackward0>)\n",
      "mark was playing with his ball and his big red ball .\n"
     ]
    }
   ],
   "source": [
    "out = model(masked_input)\n",
    "out /= 0.25\n",
    "probs = nn.functional.softmax(out, dim=-1)\n",
    "print(torch.max(probs, dim=-1).values)\n",
    "sampled_tokens = torch.multinomial(probs.view(-1, probs.size(-1)), num_samples=1).view(probs.size(0), probs.size(1))\n",
    "generated_text = torch.where(mask.to(device), sampled_tokens, generated_text)\n",
    "generated_string = vocab.idx2text(generated_text.squeeze().tolist())\n",
    "print(generated_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
