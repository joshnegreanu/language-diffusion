{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/josh/Documents/GitHub/diffusion-language-model/.conda/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "import torch\n",
    "import math\n",
    "import pandas as pd\n",
    "import random\n",
    "import wandb\n",
    "\n",
    "from torch import nn, optim\n",
    "from torch.optim.lr_scheduler import LinearLR, CosineAnnealingLR, SequentialLR\n",
    "from torch.utils.data import DataLoader, Dataset, ConcatDataset\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from staticvectors import StaticVectors\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "from models.LanguageTransformer import LanguageTransformer\n",
    "from data.LanguageDataset import PoetryDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set appropriate device\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('mps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "word2vec = StaticVectors(\"neuml/word2vec\")\n",
    "word2vec_embeddings = torch.tensor(self.word2vec.embeddings(self.tokenizer.get_vocab())).type(torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load trained model\n",
    "model_path = \"./checkpoints/diffusion-language-model/dlm-2025_12_17_12_12/4\"\n",
    "chkpt = torch.load(model_path, weights_only=False, map_location=torch.device(device))\n",
    "\n",
    "# set train, model, and generation configuration\n",
    "train_config = chkpt['train_config']\n",
    "model_config = chkpt['model_config']\n",
    "generation_config = chkpt['generation_config']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LanguageTransformer(\n",
       "  (token_embedding): Embedding(28996, 300)\n",
       "  (pos_enc): PositionalEncoding()\n",
       "  (transformer): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-11): 12 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=300, out_features=300, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=300, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=300, bias=True)\n",
       "        (norm1): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=300, out_features=2048, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=2048, out_features=28996, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load language model\n",
    "model = LanguageTransformer(\n",
    "    vocab_size=poetry_dataset.vocab_len,\n",
    "    embed_dim=model_config['emb_dim'],\n",
    "    num_layers=model_config['num_layers'],\n",
    "    num_heads=model_config['num_heads'],\n",
    "    word_emb=poetry_dataset.word2vec_embeddings\n",
    ")\n",
    "\n",
    "model.load_state_dict(chkpt['model_state_dict'])\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"Let the bird of loudest lay On the sole Arabian tree Herald sad and trumpet be,\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let the bird of loudest lay On the sole Arabian tree Herald sad and trumpet be,, the To the isto,ofy yet we Oh. mi ground the some mortal alone dream with of ride little know knows gentle Or you, ' and of I think Ande you no river was false the shall ' be love own You\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "# generated tokens (with [CLS] token)\n",
    "generated_text = poetry_dataset.tokenizer.encode(input_text, return_tensors=\"pt\", add_special_tokens=False).to(device)\n",
    "generated_text = torch.cat([torch.tensor([[poetry_dataset.tokenizer.cls_token_id]]).to(device), generated_text], dim=1)\n",
    "\n",
    "# autoregressive generation\n",
    "for _ in range(generation_config['max_length']):\n",
    "    with torch.no_grad():\n",
    "        out = model(generated_text)[:, -1, :]\n",
    "\n",
    "        # apply temperature scaling\n",
    "        out = out / generation_config['temperature']\n",
    "\n",
    "        # get probabilities\n",
    "        probs = torch.nn.functional.softmax(out, dim=-1)\n",
    "        sorted_probs, sorted_indices = torch.sort(probs, descending=True, dim=-1)\n",
    "        cumulative_probs = torch.cumsum(sorted_probs, dim=-1)\n",
    "\n",
    "        # nucleus/top-p filtering\n",
    "        sorted_probs[cumulative_probs > generation_config['top_p']] = 0\n",
    "        sorted_probs = sorted_probs / sorted_probs.sum()\n",
    "\n",
    "        # append generated token\n",
    "        next_token_id = torch.multinomial(sorted_probs, 1).item()\n",
    "        next_token = sorted_indices[0][next_token_id].reshape(1, 1)\n",
    "\n",
    "        generated_text = torch.cat([generated_text, next_token], dim=1)\n",
    "        if next_token.item() == poetry_dataset.tokenizer.eos_token_id:\n",
    "            break\n",
    "\n",
    "print(poetry_dataset.tokenizer.decode(generated_text.squeeze(0), skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
