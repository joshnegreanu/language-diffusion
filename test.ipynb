{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs/hpc/share/negreanj/miniforge/envs/torch_env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "import torch\n",
    "import math\n",
    "import pandas as pd\n",
    "import random\n",
    "import wandb\n",
    "\n",
    "from torch import nn, optim\n",
    "from torch.optim.lr_scheduler import LinearLR, CosineAnnealingLR, SequentialLR\n",
    "from torch.utils.data import DataLoader, Dataset, ConcatDataset\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from staticvectors import StaticVectors\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "from models.LanguageTransformer import LanguageTransformer\n",
    "from data.LanguageDataset import PoetryDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# set appropriate device\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemError",
     "evalue": "deallocated bytearray object has exported buffers",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mSystemError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[31mSystemError\u001b[39m: deallocated bytearray object has exported buffers"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mMemoryError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[31mMemoryError\u001b[39m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "thread '<unnamed>' (2545271) panicked at /root/.cargo/registry/src/index.crates.io-1949cf8c6b5b557f/pyo3-0.25.1/src/types/bytearray.rs:30:18:\n",
      "Python API call failed\n"
     ]
    },
    {
     "ename": "PanicException",
     "evalue": "Python API call failed",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPanicException\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m tokenizer = AutoTokenizer.from_pretrained(\u001b[33m\"\u001b[39m\u001b[33mbert-base-cased\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m word2vec = \u001b[43mStaticVectors\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mneuml/word2vec\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m word2vec_embeddings = torch.tensor(\u001b[38;5;28mself\u001b[39m.word2vec.embeddings(\u001b[38;5;28mself\u001b[39m.tokenizer.get_vocab())).type(torch.float32).to(device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/nfs/hpc/share/negreanj/miniforge/envs/torch_env/lib/python3.11/site-packages/staticvectors/model.py:38\u001b[39m, in \u001b[36mStaticVectors.__init__\u001b[39m\u001b[34m(self, path)\u001b[39m\n\u001b[32m     36\u001b[39m \u001b[38;5;66;03m# Load model, if provided\u001b[39;00m\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m path:\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/nfs/hpc/share/negreanj/miniforge/envs/torch_env/lib/python3.11/site-packages/staticvectors/model.py:111\u001b[39m, in \u001b[36mStaticVectors.load\u001b[39m\u001b[34m(self, path)\u001b[39m\n\u001b[32m    108\u001b[39m reader = StorageFactory.create(path)\n\u001b[32m    110\u001b[39m \u001b[38;5;66;03m# Load model data\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m111\u001b[39m \u001b[38;5;28mself\u001b[39m.config, \u001b[38;5;28mself\u001b[39m.vectors, \u001b[38;5;28mself\u001b[39m.quantization, \u001b[38;5;28mself\u001b[39m.weights, \u001b[38;5;28mself\u001b[39m.tokens, \u001b[38;5;28mself\u001b[39m.labels, \u001b[38;5;28mself\u001b[39m.counts = \u001b[43mreader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    113\u001b[39m \u001b[38;5;66;03m# Load additional parameters with classification models\u001b[39;00m\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.isclassification():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/nfs/hpc/share/negreanj/miniforge/envs/torch_env/lib/python3.11/site-packages/staticvectors/storage/base.py:49\u001b[39m, in \u001b[36mStorage.load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     47\u001b[39m \u001b[38;5;66;03m# Load model files\u001b[39;00m\n\u001b[32m     48\u001b[39m config = \u001b[38;5;28mself\u001b[39m.loadconfig()\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m vectors, quantization, weights = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mloadtensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     50\u001b[39m tokens, labels, counts = \u001b[38;5;28mself\u001b[39m.loadvocab()\n\u001b[32m     52\u001b[39m \u001b[38;5;66;03m# Return model data\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/nfs/hpc/share/negreanj/miniforge/envs/torch_env/lib/python3.11/site-packages/staticvectors/storage/tensors.py:21\u001b[39m, in \u001b[36mTensors.loadtensors\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mloadtensors\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m     19\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m safe_open(\u001b[38;5;28mself\u001b[39m.retrieve(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/model.safetensors\u001b[39m\u001b[33m\"\u001b[39m), framework=\u001b[33m\"\u001b[39m\u001b[33mnumpy\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m     20\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m             \u001b[43mf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvectors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m,\n\u001b[32m     22\u001b[39m             (f.get_tensor(\u001b[33m\"\u001b[39m\u001b[33mpq\u001b[39m\u001b[33m\"\u001b[39m), f.get_tensor(\u001b[33m\"\u001b[39m\u001b[33mcodewords\u001b[39m\u001b[33m\"\u001b[39m)) \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mpq\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m f.keys() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     23\u001b[39m             f.get_tensor(\u001b[33m\"\u001b[39m\u001b[33mweights\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mweights\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m f.keys() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     24\u001b[39m         )\n",
      "\u001b[31mPanicException\u001b[39m: Python API call failed"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "word2vec = StaticVectors(\"neuml/word2vec\")\n",
    "word2vec_embeddings = torch.tensor(self.word2vec.embeddings(self.tokenizer.get_vocab())).type(torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load trained model\n",
    "model_path = \"./checkpoints/diffusion-language-model/dlm-2025_12_17_13_12/9\"\n",
    "chkpt = torch.load(model_path, weights_only=False, map_location=torch.device(device))\n",
    "\n",
    "# set train, model, and generation configuration\n",
    "train_config = chkpt['train_config']\n",
    "model_config = chkpt['model_config']\n",
    "generation_config = chkpt['generation_config']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LanguageTransformer(\n",
       "  (token_embedding): Embedding(28996, 300)\n",
       "  (pos_enc): PositionalEncoding()\n",
       "  (transformer): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-11): 12 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=300, out_features=300, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=300, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=300, bias=True)\n",
       "        (norm1): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=300, out_features=2048, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=2048, out_features=28996, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load language model\n",
    "model = LanguageTransformer(\n",
    "    vocab_size=poetry_dataset.vocab_len,\n",
    "    embed_dim=model_config['emb_dim'],\n",
    "    num_layers=model_config['num_layers'],\n",
    "    num_heads=model_config['num_heads'],\n",
    "    word_emb=poetry_dataset.word2vec_embeddings\n",
    ")\n",
    "\n",
    "model.load_state_dict(chkpt['model_state_dict'])\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"Let the bird of loudest lay On the sole Arabian tree Herald sad and trumpet be,\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let the bird of loudest lay On the sole Arabian tree Herald sad and trumpet be,, the To the isto,ofy yet we Oh. mi ground the some mortal alone dream with of ride little know knows gentle Or you, ' and of I think Ande you no river was false the shall ' be love own You\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "# generated tokens (with [CLS] token)\n",
    "generated_text = poetry_dataset.tokenizer.encode(input_text, return_tensors=\"pt\", add_special_tokens=False).to(device)\n",
    "generated_text = torch.cat([torch.tensor([[poetry_dataset.tokenizer.cls_token_id]]).to(device), generated_text], dim=1)\n",
    "\n",
    "# autoregressive generation\n",
    "for _ in range(generation_config['max_length']):\n",
    "    with torch.no_grad():\n",
    "        out = model(generated_text)[:, -1, :]\n",
    "\n",
    "        # apply temperature scaling\n",
    "        out = out / generation_config['temperature']\n",
    "\n",
    "        # get probabilities\n",
    "        probs = torch.nn.functional.softmax(out, dim=-1)\n",
    "        sorted_probs, sorted_indices = torch.sort(probs, descending=True, dim=-1)\n",
    "        cumulative_probs = torch.cumsum(sorted_probs, dim=-1)\n",
    "\n",
    "        # nucleus/top-p filtering\n",
    "        sorted_probs[cumulative_probs > generation_config['top_p']] = 0\n",
    "        sorted_probs = sorted_probs / sorted_probs.sum()\n",
    "\n",
    "        # append generated token\n",
    "        next_token_id = torch.multinomial(sorted_probs, 1).item()\n",
    "        next_token = sorted_indices[0][next_token_id].reshape(1, 1)\n",
    "\n",
    "        generated_text = torch.cat([generated_text, next_token], dim=1)\n",
    "        if next_token.item() == poetry_dataset.tokenizer.eos_token_id:\n",
    "            break\n",
    "\n",
    "print(poetry_dataset.tokenizer.decode(generated_text.squeeze(0), skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
