{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs/hpc/share/negreanj/miniforge/envs/torch_env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "import torch\n",
    "import math\n",
    "import pandas as pd\n",
    "import random\n",
    "import wandb\n",
    "\n",
    "from torch import nn, optim\n",
    "from torch.optim.lr_scheduler import LinearLR, CosineAnnealingLR, SequentialLR\n",
    "from torch.utils.data import DataLoader, Dataset, ConcatDataset\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from staticvectors import StaticVectors\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "from models.LanguageTransformer import LanguageTransformer\n",
    "from data.AutoregressiveLanguage import AutoregressiveLanguageDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dynamically select device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.5921, requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# load trained model\n",
    "model_path = \"./checkpoints/autoregressive-language-model/alm-2025_12_25_20_12/alm-2025_12_25_20_12_epoch7_end.pth\"\n",
    "chkpt = torch.load(model_path, weights_only=False, map_location=torch.device(device))\n",
    "\n",
    "# get model configuration\n",
    "model_config = chkpt['model_config']\n",
    "\n",
    "# get vocabulary\n",
    "vocab = chkpt['vocab']\n",
    "print(chkpt['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LanguageTransformer(\n",
       "  (token_embedding): Embedding(9940, 256)\n",
       "  (pos_enc): PositionalEncoding()\n",
       "  (transformer): Transformer(\n",
       "    (transformer_layers): SequentialTransformerLayers(\n",
       "      (0): TransformerLayer(\n",
       "        (attn_layer): MultiheadAttention(\n",
       "          (q_linear): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (k_linear): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (v_linear): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (concat_linear): Linear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "        (feed_forward): Sequential(\n",
       "          (0): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "        )\n",
       "        (batch_norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): TransformerLayer(\n",
       "        (attn_layer): MultiheadAttention(\n",
       "          (q_linear): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (k_linear): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (v_linear): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (concat_linear): Linear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "        (feed_forward): Sequential(\n",
       "          (0): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "        )\n",
       "        (batch_norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (2): TransformerLayer(\n",
       "        (attn_layer): MultiheadAttention(\n",
       "          (q_linear): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (k_linear): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (v_linear): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (concat_linear): Linear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "        (feed_forward): Sequential(\n",
       "          (0): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "        )\n",
       "        (batch_norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (3): TransformerLayer(\n",
       "        (attn_layer): MultiheadAttention(\n",
       "          (q_linear): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (k_linear): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (v_linear): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (concat_linear): Linear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "        (feed_forward): Sequential(\n",
       "          (0): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "        )\n",
       "        (batch_norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (4): TransformerLayer(\n",
       "        (attn_layer): MultiheadAttention(\n",
       "          (q_linear): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (k_linear): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (v_linear): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (concat_linear): Linear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "        (feed_forward): Sequential(\n",
       "          (0): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "        )\n",
       "        (batch_norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (5): TransformerLayer(\n",
       "        (attn_layer): MultiheadAttention(\n",
       "          (q_linear): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (k_linear): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (v_linear): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (concat_linear): Linear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "        (feed_forward): Sequential(\n",
       "          (0): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "        )\n",
       "        (batch_norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (6): TransformerLayer(\n",
       "        (attn_layer): MultiheadAttention(\n",
       "          (q_linear): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (k_linear): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (v_linear): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (concat_linear): Linear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "        (feed_forward): Sequential(\n",
       "          (0): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "        )\n",
       "        (batch_norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (7): TransformerLayer(\n",
       "        (attn_layer): MultiheadAttention(\n",
       "          (q_linear): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (k_linear): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (v_linear): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (concat_linear): Linear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "        (feed_forward): Sequential(\n",
       "          (0): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "        )\n",
       "        (batch_norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): Linear(in_features=256, out_features=9940, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load language model\n",
    "model = LanguageTransformer(\n",
    "    vocab_size=len(vocab),\n",
    "    embed_dim=model_config['emb_dim'],\n",
    "    num_layers=model_config['num_layers'],\n",
    "    num_heads=model_config['num_heads'],\n",
    "    is_causal=False\n",
    ")\n",
    "\n",
    "model.load_state_dict(chkpt['model_state_dict'])\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = {\n",
    "    'max_len': 100,\n",
    "    'temperature': 0.5,\n",
    "    'top_p': 0.95\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"once upon a time, there was a\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fearful\n",
      ".\n",
      "because\n",
      "no\n",
      "last\n",
      "everything\n",
      "caused\n",
      "cat\n",
      "hide\n",
      "a\n",
      "zagged\n",
      "that\n",
      "max\n",
      "truth\n",
      ".\n",
      "bad\n",
      "belonged\n",
      "each\n",
      "page\n",
      "and\n",
      "it\n",
      "values\n",
      "on\n",
      "ben\n",
      "was\n",
      "lied\n",
      ".\n",
      "me\n",
      "means\n",
      ".\n",
      "bad\n",
      "left\n",
      "waves\n",
      ".\n",
      "it\n",
      "dave\n",
      "ted\n",
      "no\n",
      "you\n",
      ".\n",
      "it\n",
      "make\n",
      "sara\n",
      ".\n",
      "\n",
      "kate\n",
      "â€œthen\n",
      "sam\n",
      "wounds\n",
      "sam\n",
      "fall\n",
      "sobs\n",
      "means\n",
      "morning\n",
      "came\n",
      "tom\n",
      "sara\n",
      ".\n",
      "ice\n",
      ".\n",
      "billy\n",
      "stitch\n",
      ".\n",
      "trophy\n",
      ".\n",
      "jelly\n",
      "eye\n",
      "loves\n",
      "hamster\n",
      "even\n",
      "love\n",
      "timmy\n",
      ".\n",
      "bacon\n",
      "gum\n",
      "water\n",
      "cried\n",
      "molly\n",
      "poison\n",
      "say\n",
      "pain\n",
      "gorilla\n",
      "winter\n",
      "days\n",
      "-\n",
      "never\n",
      "his\n",
      "lily\n",
      ".\n",
      "pat\n",
      "lila\n",
      "kiss\n",
      "skips\n",
      "suggest\n",
      "to\n",
      "god\n",
      "for\n",
      "billy\n",
      "things\n",
      "the\n",
      "once upon a time, there was a fearful. because no last everything caused cat hide a zagged that max truth. bad belonged each page and it values on ben was lied. me means. bad left waves. it dave ted no you. it make sara.  kate â€œthen sam wounds sam fall sobs means morning came tom sara. ice. billy stitch. trophy. jelly eye loves hamster even love timmy. bacon gum water cried molly poison say pain gorilla winter days - never his lily. pat lila kiss skips suggest to god for billy things\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# prepare input\n",
    "generated_text = torch.tensor([vocab.word2idx['<s>']] + vocab.text2idx(input_text), device=device).unsqueeze(0)\n",
    "\n",
    "# autoregressive generation\n",
    "for _ in range(generation_config['max_len']):\n",
    "    with torch.no_grad():\n",
    "        out = model(generated_text)[:, -1, :]\n",
    "\n",
    "        # apply temperature scaling\n",
    "        # out = out / generation_config['temperature']\n",
    "\n",
    "        # get probabilities\n",
    "        probs = torch.nn.functional.softmax(out, dim=1)\n",
    "\n",
    "        # nucleus/top-p filtering\n",
    "        # sorted_probs, sorted_indices = torch.sort(probs, descending=True, dim=1)\n",
    "        # cumulative_probs = torch.cumsum(sorted_probs, dim=1)\n",
    "        \n",
    "        # nucleus coverage\n",
    "        # nucleus = cumulative_probs < generation_config['top_p']\n",
    "        # nucleus_size = torch.sum(nucleus).item()\n",
    "        # nucleus[0,nucleus_size] = True\n",
    "\n",
    "        # apply coverage\n",
    "        # nucleus_probs = torch.zeros_like(probs)\n",
    "        # nucleus_probs = torch.where(nucleus, sorted_probs, sorted_probs)\n",
    "        # nucleus_probs /= torch.sum(nucleus_probs).item()\n",
    "\n",
    "        # append generated token\n",
    "        # next_token_id = torch.multinomial(nucleus_probs, 1).item()\n",
    "        # next_token = sorted_indices[0][next_token_id].reshape(1, 1)\n",
    "\n",
    "        # next_token = torch.argmax(out)\n",
    "        next_token = torch.multinomial(probs, 1)\n",
    "        print(vocab.idx2word[next_token.item()])\n",
    "\n",
    "        generated_text = torch.cat([generated_text, next_token], dim=1)\n",
    "        if next_token.item() == vocab.word2idx['</s>']:\n",
    "            break\n",
    "\n",
    "# decode generated tokens\n",
    "generated_string = vocab.idx2text(generated_text.squeeze().tolist()[1:-1])\n",
    "generated_string = re.sub(r'\\s+([.,!?])', r'\\1', generated_string)\n",
    "print(generated_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n"
     ]
    }
   ],
   "source": [
    "input_text = \"bob and sally\"\n",
    "generated_text = torch.tensor([vocab.word2idx['<s>']] + vocab.text2idx(input_text), device=device).unsqueeze(0)\n",
    "out = model(generated_text)[:, -1, :]\n",
    "print(vocab.idx2word[torch.argmax(out).item()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
