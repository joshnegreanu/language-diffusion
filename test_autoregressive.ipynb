{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/josh/miniconda3/envs/nlp-hw2/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "import torch\n",
    "import math\n",
    "import pandas as pd\n",
    "import random\n",
    "import wandb\n",
    "\n",
    "from torch import nn, optim\n",
    "from torch.optim.lr_scheduler import LinearLR, CosineAnnealingLR, SequentialLR\n",
    "from torch.utils.data import DataLoader, Dataset, ConcatDataset\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from staticvectors import StaticVectors\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "from models.LanguageTransformer import LanguageTransformer\n",
    "from data.AutoregressiveLanguage import AutoregressiveLanguageDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dynamically select device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LanguageTransformer(\n",
    "    vocab_size=100,\n",
    "    embed_dim=256,\n",
    "    num_layers=4,\n",
    "    num_heads=4,\n",
    "    is_causal=True\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load trained model\n",
    "model_path = \"/Users/josh/Documents/GitHub/diffusion-language-model/checkpoints/autoregressive-language-model/alm-2025_12_29_21_12_epoch1_end.pth\"\n",
    "chkpt = torch.load(model_path, weights_only=False, map_location=torch.device(device))\n",
    "\n",
    "# get model configuration\n",
    "model_config = chkpt['model_config']\n",
    "\n",
    "# get vocabulary\n",
    "vocab = chkpt['vocab']\n",
    "model = LanguageTransformer(\n",
    "vocab_size=len(vocab),\n",
    "embed_dim=model_config['emb_dim'],\n",
    "num_layers=model_config['num_layers'],\n",
    "num_heads=model_config['num_heads'],\n",
    "is_causal=True\n",
    ")\n",
    "\n",
    "model.load_state_dict(chkpt['model_state_dict'])\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = torch.randint(0, 100, (32, 10)).to(device)\n",
    "out = model(seq)\n",
    "assert out.shape == (32, 10, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = {\n",
    "    'max_len': 100,\n",
    "    'temperature': 0.25,\n",
    "    'top_p': 0.95\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"once upon a time, there\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# prepare input\n",
    "generated_text = torch.tensor([vocab.word2idx['<s>']] + vocab.text2idx(input_text), device=device).unsqueeze(0)\n",
    "\n",
    "# autoregressive generation\n",
    "for _ in range(generation_config['max_len']):\n",
    "    with torch.no_grad():\n",
    "        out = model(generated_text)[:, -1, :]\n",
    "\n",
    "        # apply temperature scaling\n",
    "        out = out / generation_config['temperature']\n",
    "\n",
    "        # get probabilities\n",
    "        probs = torch.nn.functional.softmax(out, dim=1)\n",
    "\n",
    "        # nucleus/top-p filtering\n",
    "        sorted_probs, sorted_indices = torch.sort(probs, descending=True, dim=1)\n",
    "        cumulative_probs = torch.cumsum(sorted_probs, dim=1)\n",
    "        \n",
    "        # nucleus coverage\n",
    "        nucleus = cumulative_probs < generation_config['top_p']\n",
    "        nucleus_size = torch.sum(nucleus).item()\n",
    "        nucleus[0,nucleus_size] = True\n",
    "\n",
    "        # apply coverage\n",
    "        nucleus_probs = torch.zeros_like(probs)\n",
    "        nucleus_probs = torch.where(nucleus, sorted_probs, sorted_probs)\n",
    "        nucleus_probs /= torch.sum(nucleus_probs).item()\n",
    "\n",
    "        # append generated token\n",
    "        next_token_id = torch.multinomial(nucleus_probs, 1).item()\n",
    "        next_token = sorted_indices[0][next_token_id].reshape(1, 1)\n",
    "\n",
    "        # next_token = torch.argmax(out, dim=1).reshape(1, 1)\n",
    "\n",
    "        generated_text = torch.cat([generated_text, next_token], dim=1)\n",
    "        if next_token.item() == vocab.word2idx['</s>']:\n",
    "            break\n",
    "\n",
    "# decode generated tokens\n",
    "generated_string = vocab.idx2text(generated_text.squeeze().tolist()[1:-1])\n",
    "generated_string = re.sub(r'\\s+([.,!?])', r'\\1', generated_string)\n",
    "print(generated_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"once upon a time\"\n",
    "generated_text = torch.tensor([vocab.word2idx['<s>']] + vocab.text2idx(input_text), device=device).unsqueeze(0)\n",
    "out = model(generated_text)[:, :-1, :]\n",
    "print(vocab.idx2text(torch.argmax(out, dim=-1).squeeze().tolist()[1:]))\n",
    "# print(vocab.idx2word[torch.argmax(out).item()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
