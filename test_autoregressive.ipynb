{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs/hpc/share/negreanj/miniforge/envs/torch_env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "import torch\n",
    "import math\n",
    "import pandas as pd\n",
    "import random\n",
    "import wandb\n",
    "\n",
    "from torch import nn, optim\n",
    "from torch.optim.lr_scheduler import LinearLR, CosineAnnealingLR, SequentialLR\n",
    "from torch.utils.data import DataLoader, Dataset, ConcatDataset\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from staticvectors import StaticVectors\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "from models.LanguageTransformer import LanguageTransformer\n",
    "from data.AutoregressiveLanguage import AutoregressiveLanguageDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dynamically select device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.8062, requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# load trained model\n",
    "model_path = \"./checkpoints/autoregressive-language-model/alm-2025_12_25_15_12/alm-2025_12_25_15_12_epoch9_end.pth\"\n",
    "chkpt = torch.load(model_path, weights_only=False, map_location=torch.device(device))\n",
    "\n",
    "# get model configuration\n",
    "model_config = chkpt['model_config']\n",
    "\n",
    "# get vocabulary\n",
    "vocab = chkpt['vocab']\n",
    "print(chkpt['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LanguageTransformer(\n",
       "  (token_embedding): Embedding(6027, 256)\n",
       "  (pos_enc): PositionalEncoding()\n",
       "  (transformer): Transformer(\n",
       "    (transformer_layers): SequentialTransformerLayers(\n",
       "      (0): TransformerLayer(\n",
       "        (attn_layer): MultiheadAttention(\n",
       "          (q_linear): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (k_linear): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (v_linear): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (concat_linear): Linear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "        (feed_forward): Sequential(\n",
       "          (0): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "        )\n",
       "        (batch_norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): TransformerLayer(\n",
       "        (attn_layer): MultiheadAttention(\n",
       "          (q_linear): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (k_linear): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (v_linear): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (concat_linear): Linear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "        (feed_forward): Sequential(\n",
       "          (0): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "        )\n",
       "        (batch_norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (2): TransformerLayer(\n",
       "        (attn_layer): MultiheadAttention(\n",
       "          (q_linear): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (k_linear): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (v_linear): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (concat_linear): Linear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "        (feed_forward): Sequential(\n",
       "          (0): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "        )\n",
       "        (batch_norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (3): TransformerLayer(\n",
       "        (attn_layer): MultiheadAttention(\n",
       "          (q_linear): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (k_linear): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (v_linear): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (concat_linear): Linear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "        (feed_forward): Sequential(\n",
       "          (0): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "        )\n",
       "        (batch_norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (4): TransformerLayer(\n",
       "        (attn_layer): MultiheadAttention(\n",
       "          (q_linear): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (k_linear): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (v_linear): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (concat_linear): Linear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "        (feed_forward): Sequential(\n",
       "          (0): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "        )\n",
       "        (batch_norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (5): TransformerLayer(\n",
       "        (attn_layer): MultiheadAttention(\n",
       "          (q_linear): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (k_linear): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (v_linear): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (concat_linear): Linear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "        (feed_forward): Sequential(\n",
       "          (0): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "        )\n",
       "        (batch_norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (6): TransformerLayer(\n",
       "        (attn_layer): MultiheadAttention(\n",
       "          (q_linear): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (k_linear): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (v_linear): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (concat_linear): Linear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "        (feed_forward): Sequential(\n",
       "          (0): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "        )\n",
       "        (batch_norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (7): TransformerLayer(\n",
       "        (attn_layer): MultiheadAttention(\n",
       "          (q_linear): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (k_linear): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (v_linear): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (concat_linear): Linear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "        (feed_forward): Sequential(\n",
       "          (0): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "        )\n",
       "        (batch_norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): Linear(in_features=256, out_features=6027, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load language model\n",
    "model = LanguageTransformer(\n",
    "    vocab_size=len(vocab),\n",
    "    embed_dim=model_config['emb_dim'],\n",
    "    num_layers=model_config['num_layers'],\n",
    "    num_heads=model_config['num_heads']\n",
    ")\n",
    "\n",
    "model.load_state_dict(chkpt['model_state_dict'])\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = {\n",
    "    'max_len': 100,\n",
    "    'temperature': 0.25,\n",
    "    'top_p': 0.95\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"once upon a time,\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "once upon a time, there was a teddy. the teddy was a box. the box was, and the box was a box. the box was a box. the box was the box. the box was the box. the box was the box. the box was the box. the box was the box. the box was the box. the box was the box. the box was the box. the box was the box. the box was the box. the box was the box. the box was the box\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# prepare input\n",
    "generated_text = torch.tensor([vocab.word2idx['<s>']] + vocab.text2idx(input_text), device=device).unsqueeze(0)\n",
    "\n",
    "# autoregressive generation\n",
    "for _ in range(generation_config['max_len']):\n",
    "    with torch.no_grad():\n",
    "        out = model(generated_text)[:, -1, :]\n",
    "\n",
    "        # apply temperature scaling\n",
    "        out = out / generation_config['temperature']\n",
    "\n",
    "        # get probabilities\n",
    "        probs = torch.nn.functional.softmax(out, dim=1)\n",
    "\n",
    "        # nucleus/top-p filtering\n",
    "        sorted_probs, sorted_indices = torch.sort(probs, descending=True, dim=1)\n",
    "        cumulative_probs = torch.cumsum(sorted_probs, dim=1)\n",
    "        \n",
    "        # nucleus coverage\n",
    "        nucleus = cumulative_probs < generation_config['top_p']\n",
    "        nucleus_size = torch.sum(nucleus).item()\n",
    "        nucleus[0,nucleus_size] = True\n",
    "\n",
    "        # apply coverage\n",
    "        nucleus_probs = torch.zeros_like(probs)\n",
    "        nucleus_probs = torch.where(nucleus, sorted_probs, sorted_probs)\n",
    "        nucleus_probs /= torch.sum(nucleus_probs).item()\n",
    "\n",
    "        # append generated token\n",
    "        next_token_id = torch.multinomial(nucleus_probs, 1).item()\n",
    "        next_token = sorted_indices[0][next_token_id].reshape(1, 1)\n",
    "\n",
    "        generated_text = torch.cat([generated_text, next_token], dim=1)\n",
    "        if next_token.item() == vocab.word2idx['</s>']:\n",
    "            break\n",
    "\n",
    "# decode generated tokens\n",
    "generated_string = vocab.idx2text(generated_text.squeeze().tolist()[1:-1])\n",
    "generated_string = re.sub(r'\\s+([.,!?])', r'\\1', generated_string)\n",
    "print(generated_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
