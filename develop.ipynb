{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "import torch\n",
    "import math\n",
    "import pandas as pd\n",
    "import random\n",
    "import wandb\n",
    "\n",
    "from torch import nn, optim\n",
    "from torch.optim.lr_scheduler import LinearLR, CosineAnnealingLR, SequentialLR\n",
    "from torch.utils.data import DataLoader, Dataset, ConcatDataset\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from staticvectors import StaticVectors\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set appropriate device\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('mps')\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "  def __init__(self, embed_dim):\n",
    "    super().__init__()\n",
    "\n",
    "    self.embed_dim = embed_dim\n",
    "\n",
    "  def forward(self, x):\n",
    "    # batch size and sequence length bookkeeping\n",
    "    batch_size = x.shape[0]\n",
    "    seq_len = x.shape[1]\n",
    "\n",
    "    # initialize positional encoding\n",
    "    pe = torch.zeros(1, seq_len, self.embed_dim).to(x.device)\n",
    "\n",
    "    # calculate encoding term\n",
    "    pos = torch.arange(0, seq_len, dtype=torch.float32)\n",
    "    enc = torch.exp((-math.log(10000.0)) * (torch.arange(0, self.embed_dim, step=2, dtype=torch.float32) / self.embed_dim))\n",
    "\n",
    "    # calculate positional encoding\n",
    "    prod = torch.outer(pos, enc)\n",
    "    pe[0, :, 0::2] = torch.sin(prod)\n",
    "    pe[0, :, 1::2] = torch.cos(prod)\n",
    "    pe = pe.expand(batch_size, -1, -1)\n",
    "\n",
    "    # apply as residual\n",
    "    x = x + pe\n",
    "    return x\n",
    "\n",
    "\n",
    "class LanguageTransformer(nn.Module):\n",
    "\n",
    "  def __init__(\n",
    "    self,\n",
    "    vocab_size,\n",
    "    embed_dim,\n",
    "    num_layers,\n",
    "    num_heads,\n",
    "    word_emb=None\n",
    "  ):\n",
    "    super().__init__()\n",
    "\n",
    "    # learned (or given) vector embeddings\n",
    "    if word_emb is not None:\n",
    "      self.token_embedding = nn.Embedding.from_pretrained(word_emb)\n",
    "\n",
    "      # freeze embeddings besides custom tokens\n",
    "      self.token_embedding.weight.data[:].requires_grad_(False)\n",
    "    else:\n",
    "      self.token_embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embed_dim)\n",
    "\n",
    "    # positional encodings\n",
    "    self.pos_enc = PositionalEncoding(embed_dim=embed_dim)\n",
    "\n",
    "    # prepare single transformer layer with multiheaded attention\n",
    "    transformer_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads, batch_first=True)\n",
    "\n",
    "    # create transformer with multiple layers\n",
    "    self.transformer = nn.TransformerEncoder(encoder_layer=transformer_layer, num_layers=num_layers)\n",
    "\n",
    "    # vocab classifier\n",
    "    self.classifier = nn.Sequential(\n",
    "      nn.Linear(in_features=embed_dim, out_features=1024),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(in_features=1024, out_features=vocab_size)\n",
    "    )\n",
    "    \n",
    "  def forward(self, seq):\n",
    "    # get lengths of sequences\n",
    "    seq_len = seq.shape[1]\n",
    "\n",
    "    # embed sequence w poisitional encodings\n",
    "    seq_embed = self.token_embedding(seq)\n",
    "    seq_embed = self.pos_enc(seq_embed)\n",
    "\n",
    "    # generate custom prefixed causal mask\n",
    "    mask = torch.zeros(seq_len, seq_len, dtype=torch.float32).to(device)\n",
    "    seq_out = self.transformer(src=seq_embed, mask=mask)\n",
    "\n",
    "    # classify target sequence output into target vocabulary\n",
    "    out = self.classifier(seq_out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bert tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word2vec embeddings (300 dim)\n",
    "word2vec = StaticVectors(\"neuml/word2vec\")\n",
    "word2vec_embeddings = torch.tensor(word2vec.embeddings(tokenizer.get_vocab())).type(torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_config = {\n",
    "    'bs': 16,\n",
    "    'lr': 0.001,\n",
    "    'weight_decay': 0.00001,\n",
    "    'max_epochs': 10\n",
    "}\n",
    "\n",
    "# model configuration\n",
    "model_config = {\n",
    "    'emb_dim': word2vec_embeddings.shape[1],\n",
    "    'num_layers': 4,\n",
    "    'num_heads': 4\n",
    "}\n",
    "\n",
    "vocab_len = len(tokenizer.get_vocab())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let the bird of loudest lay On the sole Arabian tree Herald sad and trumpet be, To whose sound chaste wings obey. But thou shrieking harbinger, Foul precurrer of the fiend, Augur of the fever ' s end, To this troop come thou not near. From this session interdict Every fowl of tyrant wing, Save the eagle, feather ' d king ; Keep the obsequy so strict. Let the priest in surplice white, That defunctive music can, Be the death - divining swan, Lest the requiem lack his right. And thou treble - dated crow, That thy sable gender mak ' st With the breath thou giv ' st and tak ' st, ' Mongst our mourners shalt thou go. Here the anthem doth commence : Love and constancy is dead ; Phoenix and the Turtle fled In a mutual flame from hence. So they lov ' d, as love in twain Had the essence but in one ; Two distincts, division none : Number there in love was slain. Hearts remote, yet not asunder ; Distance and no space was seen ' Twixt this Turtle and his queen : But in them it were a wonder. So between them love did shine That the Turtle saw his right Flaming in the Phoenix ' sight : Either was the other ' s mine. Property was thus appalled That the self was not the same ; Single nature ' s double name Neither two nor one was called. Reason, in itself confounded, Saw division grow together, To themselves yet either neither, Simple were so well compounded ; That it cried, \" How true a twain Seemeth this concordant one! Love has reason, reason none, If what parts can so remain. \" Whereupon it made this threne To the Phoenix and the Dove, Co - supremes and stars of love, As chorus to their tragic scene : threnos Beauty, truth, and rarity, Grace in all simplicity, Here enclos ' d, in cinders lie. Death is now the Phoenix ' nest, And the Turtle ' s loyal breast To eternity doth rest, Leaving no posterity : ' Twas not their infirmity, It was married chastity. Truth may seem but cannot be ; Beauty brag but ' tis not she ; Truth and beauty buried be. To this urn let those repair That are either true or fair ; For these dead birds sigh a prayer.\n"
     ]
    }
   ],
   "source": [
    "# load huggingface dataset\n",
    "poetry_dataset = load_dataset(\"merve/poetry\")\n",
    "\n",
    "# tokenize\n",
    "dataset_tokens = tokenizer(list(poetry_dataset['train']['content']), padding='max_length', max_length=2048, truncation=True, return_tensors=\"pt\", add_special_tokens=True)\n",
    "inputs = dataset_tokens.input_ids[:, :-1]\n",
    "labels = dataset_tokens.input_ids[:, 1:]\n",
    "\n",
    "tokenized_dataset = torch.utils.data.TensorDataset(inputs, labels)\n",
    "\n",
    "# quick check on decode/encode\n",
    "print(tokenizer.decode(tokenized_dataset[0][0], skip_special_tokens=True))\n",
    "\n",
    "# create dataloader\n",
    "train_loader = DataLoader(tokenized_dataset, batch_size=train_config['bs'], num_workers=4, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed dry run!\n"
     ]
    }
   ],
   "source": [
    "# create language model\n",
    "model = LanguageTransformer(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    embed_dim=model_config['emb_dim'],\n",
    "    num_layers=model_config['num_layers'],\n",
    "    num_heads=model_config['num_heads'],\n",
    "    word_emb=word2vec_embeddings\n",
    "    ).to(device)\n",
    "\n",
    "# quick dry run\n",
    "bs = train_config[\"bs\"]\n",
    "seq_len = random.randint(0, 100)\n",
    "\n",
    "seq = torch.randint(0, vocab_len, (bs, seq_len)).to(device)\n",
    "out = model(seq)\n",
    "\n",
    "assert out.shape == (bs, seq_len, vocab_len)\n",
    "print(\"Passed dry run!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up wandb\n",
    "# now = datetime.now()\n",
    "# run_name = \"dlm-\" + now.strftime(\"%Y_%m_%d_%H_%m\")\n",
    "\n",
    "# initialize wandb session\n",
    "# wandb.login()\n",
    "# wandb.init(project=\"diffusion-language-model\", name=run_name, config=train_config)\n",
    "\n",
    "# custom optimizer\n",
    "optimizer = optim.AdamW(model.parameters(), lr=train_config['lr'], weight_decay=train_config['weight_decay']) \n",
    "\n",
    "# define warmup and cooldown epochs\n",
    "warmup_epochs = int(train_config['max_epochs'] / 10)\n",
    "cooldown_epochs = train_config['max_epochs'] - warmup_epochs\n",
    "\n",
    "# epoch length\n",
    "epoch_len = len(dataset_tokenized) // train_config['bs']\n",
    "\n",
    "# construct linear warmup and cosine annealing scheduler\n",
    "linear = LinearLR(optimizer, start_factor=0.25, end_factor=1.0, total_iters=warmup_epochs*epoch_len)\n",
    "cosine = CosineAnnealingLR(optimizer, T_max=cooldown_epochs*epoch_len, eta_min=1e-6)\n",
    "scheduler = SequentialLR(optimizer, schedulers=[linear, cosine], milestones=[warmup_epochs*epoch_len])\n",
    "\n",
    "# set up cross entropy loss for transformer output\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "\n",
    "# set up progress bar\n",
    "pbar = tqdm(total=(train_config['max_epochs'])*epoch_len, desc=\"Training Iterations\", unit=\"batch\")\n",
    "\n",
    "# main training loop\n",
    "# iteration = 0\n",
    "for epoch in range(train_config['max_epochs']):\n",
    "    # set model to train\n",
    "    model.train()\n",
    "\n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        # log lr for each epoch\n",
    "        # wandb.log({'learning-rate': scheduler.get_last_lr()[0]}, step=iteration)\n",
    "\n",
    "        inputs = batch[0].to(device)\n",
    "        labels = batch[1].to(device)\n",
    "        break\n",
    "\n",
    "        # run through model\n",
    "        out = model(inputs)\n",
    "        print(out.shape)\n",
    "\n",
    "        # compute loss\n",
    "        loss = criterion(out, output_tokens)\n",
    "        print(loss.item())\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # log loss/train and accuracy/train per batch\n",
    "        # wandb.log({\"loss\": loss.item()}, step=iteration)\n",
    "        \n",
    "        pbar.update(1)\n",
    "        # iteration += 1\n",
    "\n",
    "        # step through scheduler\n",
    "        scheduler.step()\n",
    "\n",
    "# wandb.finish()\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
